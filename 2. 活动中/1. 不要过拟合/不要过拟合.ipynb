{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明：这个主题有意思的是，只有250个训练数据，而测试集差不多有20000个，需要从这250个数据中得到一个不过拟合模型来进行预测，这是很有挑战的。\n",
    "\n",
    "总共有300个特征，我们可以先来看下数据的具体情况！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T06:42:58.493215Z",
     "start_time": "2019-03-05T06:42:56.499233Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T06:42:58.527596Z",
     "start_time": "2019-03-05T06:42:58.498534Z"
    }
   },
   "outputs": [],
   "source": [
    "def loaddata(file, train=True):\n",
    "    if train:\n",
    "        data_train = pd.read_csv(file)\n",
    "        X_train = data_train[data_train.columns[2:]]\n",
    "        y_train = data_train[data_train.columns[1]]\n",
    "        return X_train, y_train\n",
    "    else:\n",
    "        data_test = pd.read_csv(file)\n",
    "        X_test = data_test[data_test.columns[1:]]\n",
    "        return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T06:43:03.976364Z",
     "start_time": "2019-03-05T06:42:58.531739Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = loaddata('./数据集/train.csv')\n",
    "X_test = loaddata('./数据集/test.csv', train=False)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T06:43:03.989494Z",
     "start_time": "2019-03-05T06:43:03.981258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-05T06:42:56.535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-05T06:42:56.546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19750, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些数据，其实都已经做了标准化处理，这边关系的就是通过何种技巧来使得得到的模型不过拟合！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先使用简单的逻辑回归来试试~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-05T06:42:56.555Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-05T06:42:56.564Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain, Xval, ytrain, yval = train_test_split(X_train, y_train, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-05T06:42:56.574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.745011086474501"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='lbfgs')\n",
    "gscv_lr = GridSearchCV(lr, {'tol': [1e-4, 1e-5, 1e-6, 1e-7, 1e-8],\n",
    "                            'C': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "                            'random_state': range(101),\n",
    "                            'max_iter': [100, 200, 500, 1000, 2000, 5000]}, cv=10, n_jobs=-1, iid=False)\n",
    "gscv_lr.fit(Xtrain, ytrain)\n",
    "lr = LogisticRegression(solver='lbfgs', tol=gscv_lr.best_params_['tol'], C=gscv_lr.best_params_['C'],\n",
    "                        random_state=gscv_lr.best_params_['random_state'], max_iter=gscv_lr.best_params_['max_iter'])\n",
    "lr.fit(Xtrain, ytrain)\n",
    "ypred = lr.predict(Xval)\n",
    "roc_auc_score(yval, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T05:28:32.019490Z",
     "start_time": "2019-02-28T05:28:32.013026Z"
    }
   },
   "source": [
    "再来看下SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-05T06:42:56.582Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-05T06:42:56.593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7084257206208426"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_c = SVC(gamma='scale')\n",
    "gscv_c = GridSearchCV(svm_c, {'C': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "                              'kernel': ['linear', 'rbf'],\n",
    "                              'tol': [1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8],\n",
    "                              'random_state': range(101)}, cv=10, n_jobs=-1, iid=False)\n",
    "gscv_c.fit(Xtrain, ytrain)\n",
    "svm_c = SVC(gamma='scale', C=gscv_c.best_params_['C'], kernel=gscv_c.best_params_['kernel'],\n",
    "            tol=gscv_c.best_params_['tol'], random_state=gscv_c.best_params_['random_state'])\n",
    "svm_c.fit(Xtrain, ytrain)\n",
    "ypred = svm_c.predict(Xval)\n",
    "roc_auc_score(yval, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再用一下随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-05T06:42:56.602Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-05T06:42:56.615Z"
    }
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "gscv_rfc = GridSearchCV(rfc, {'n_estimators': [50, 80, 100, 200],\n",
    "                              'max_depth': [3, 10, 30, 50],\n",
    "                              'min_samples_split': [2, 5, 10],\n",
    "                              'min_samples_leaf': [1, 2, 5],\n",
    "                              'random_state': range(101)}, cv=10, n_jobs=-1, iid=False)\n",
    "gscv_rfc.fit(Xtrain, ytrain)\n",
    "rfc = RandomForestClassifier(n_estimators=gscv_rfc.best_params_['n_estimators'],\n",
    "                             max_depth=gscv_rfc.best_params_['max_depth'],\n",
    "                             min_samples_split=gscv_rfc.best_params_['min_samples_split'],\n",
    "                             min_samples_leaf=gscv_rfc.best_params_['min_samples_leaf'],\n",
    "                             random_state=gscv_rfc.best_params_['random_state'])\n",
    "rfc.fit(Xtrain, ytrain)\n",
    "ypred = rfc.predict(Xval)\n",
    "roc_auc_score(yval, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-05T06:42:56.625Z"
    }
   },
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "gscv_gbc = GridSearchCV(gbc, {'n_estimators': [50, 80, 100, 200],\n",
    "                              'max_depth': [3, 5, 10, 30, 50],\n",
    "                              'learning_rate': [0.05, 0.1, 0.2],\n",
    "                              'min_samples_split': [2, 5, 10],\n",
    "                              'min_samples_leaf': [1, 2, 5],\n",
    "                              'subsample': [0.6, 0.8, 1.0],\n",
    "                              'random_state': range(101)}, cv=10, n_jobs=-1, iid=False)\n",
    "gscv_gbc.fit(Xtrain, ytrain)\n",
    "gbc = GradientBoostingClassifier(loss='exponential',\n",
    "                                 n_estimators=gscv_gbc.best_params_['n_estimators'],\n",
    "                                 max_depth=gscv_gbc.best_params_['max_depth'],\n",
    "                                 learning_rate=gscv_gbc.best_params_['learning_rate'],\n",
    "                                 min_samples_split=gscv_gbc.best_params_['min_samples_split'],\n",
    "                                 min_samples_leaf=gscv_gbc.best_params_['min_samples_leaf'],\n",
    "                                 subsample=gscv_gbc.best_params_['subsample'],\n",
    "                                 random_state=gscv_gbc.best_params_['random_state'])\n",
    "gbc.fit(Xtrain, ytrain)\n",
    "ypred = gbc.predict(Xval)\n",
    "roc_auc_score(yval, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，再构造以下DNN来试一下，看看有什么不同没"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-05T06:42:56.636Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import models, layers, regularizers, initializers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-05T06:42:56.645Z"
    }
   },
   "outputs": [],
   "source": [
    "max_val_acc = []\n",
    "max_auc_score = []\n",
    "params = []\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(patience=50) \n",
    "\n",
    "for hidden_size in range(32, 512, 32):\n",
    "    for activation in ['relu', 'elu', 'selu']:\n",
    "        for l1 in [0.001, 0.005, 0.01, 0.02, 0.05, 0.1]:\n",
    "            for l2 in [0.001, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2]:\n",
    "                for epochs in range(10, 200, 10):\n",
    "                    for batch_size in range(10, 50, 10):\n",
    "                        model = models.Sequential()\n",
    "                        model.add(layers.Dense(hidden_size, activation=activation, kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "                        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "                        model.compile(optimizer='adam', metrics=['accuracy'], loss='binary_crossentropy')\n",
    "                        history = model.fit(Xtrain, ytrain, epochs=epochs, batch_size=batch_size, validation_data=(Xval, yval), verbose=0, callbacks=[early_stopping])\n",
    "                        ypred = model.predict(Xval)\n",
    "                                            \n",
    "                        max_val_acc.append(max(history.history['val_acc']))\n",
    "                        max_auc_score.append(roc_auc_score(yval, ypred))\n",
    "                        params.append([hidden_size, activation, l1, l2, epochs, batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-05T06:42:56.654Z"
    }
   },
   "outputs": [],
   "source": [
    "max_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-05T06:42:56.666Z"
    }
   },
   "outputs": [],
   "source": [
    "max_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-05T06:42:56.674Z"
    }
   },
   "outputs": [],
   "source": [
    "max(max_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-05T06:42:56.683Z"
    }
   },
   "outputs": [],
   "source": [
    "best_params = params[np.argmax(max_auc_score)]\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(best_params[0], activation=best_params[1], kernel_regularizer=regularizers.l1_l2(l1=best_params[2], l2=best_params[3])))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', metrics=['accuracy'], loss='binary_crossentropy')\n",
    "history = model.fit(Xtrain, ytrain, epochs=best_params[4], batch_size=best_params[5], validation_data=(Xval, yval), callbacks=[early_stopping])\n",
    "ypred = model.predict(Xval)\n",
    "roc_auc_score(yval, ypred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
