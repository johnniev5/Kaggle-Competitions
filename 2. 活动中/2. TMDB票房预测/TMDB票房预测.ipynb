{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明：训练数据集有3000部电影，包含\"id, cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries\"这些特征。其中会有同名电影的翻拍，这应该被看做是不同的电影。测试集有4398部电影。\n",
    "\n",
    "这边我们加入5000部同样是来自TMDB的数据集，以增加预测的准确性（因为训练集实在是太少了！）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先加载数据看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:49.440831Z",
     "start_time": "2019-03-03T11:58:47.543550Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:49.451246Z",
     "start_time": "2019-03-03T11:58:49.444567Z"
    }
   },
   "outputs": [],
   "source": [
    "def loaddata(file, train=True):\n",
    "    if train:\n",
    "        data_train = pd.read_csv(file)\n",
    "        X_train = data_train[data_train.columns[1:-1]]\n",
    "        y_train = data_train[data_train.columns[-1]]\n",
    "        return X_train, y_train\n",
    "    else:\n",
    "        data_test = pd.read_csv(file)\n",
    "        X_test = data_test[data_test.columns[1:]]\n",
    "        return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:51.173584Z",
     "start_time": "2019-03-03T11:58:49.456061Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_orignal, y_train_orignal = loaddata('./数据集/train.csv')\n",
    "X_test = loaddata('./数据集/test.csv', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:51.189362Z",
     "start_time": "2019-03-03T11:58:51.178630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 21)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_orignal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:51.205136Z",
     "start_time": "2019-03-03T11:58:51.196567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4398, 21)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:51.219413Z",
     "start_time": "2019-03-03T11:58:51.208668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['belongs_to_collection', 'budget', 'genres', 'homepage', 'imdb_id',\n",
       "       'original_language', 'original_title', 'overview', 'popularity',\n",
       "       'poster_path', 'production_companies', 'production_countries',\n",
       "       'release_date', 'runtime', 'spoken_languages', 'status', 'tagline',\n",
       "       'title', 'keywords', 'cast', 'crew'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_orignal.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:51.231708Z",
     "start_time": "2019-03-03T11:58:51.222903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_orignal.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:51.270397Z",
     "start_time": "2019-03-03T11:58:51.234759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>poster_path</th>\n",
       "      <th>...</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'id': 313576, 'name': 'Hot Tub Time Machine ...</td>\n",
       "      <td>14000000</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt2637294</td>\n",
       "      <td>en</td>\n",
       "      <td>Hot Tub Time Machine 2</td>\n",
       "      <td>When Lou, who has become the \"father of the In...</td>\n",
       "      <td>6.575393</td>\n",
       "      <td>/tQtWuwvMf0hCc2QR2tkolwl7c3c.jpg</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
       "      <td>2/20/15</td>\n",
       "      <td>93.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>The Laws of Space and Time are About to be Vio...</td>\n",
       "      <td>Hot Tub Time Machine 2</td>\n",
       "      <td>[{'id': 4379, 'name': 'time travel'}, {'id': 9...</td>\n",
       "      <td>[{'cast_id': 4, 'character': 'Lou', 'credit_id...</td>\n",
       "      <td>[{'credit_id': '59ac067c92514107af02c8c8', 'de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               belongs_to_collection    budget  \\\n",
       "0  [{'id': 313576, 'name': 'Hot Tub Time Machine ...  14000000   \n",
       "\n",
       "                           genres homepage    imdb_id original_language  \\\n",
       "0  [{'id': 35, 'name': 'Comedy'}]      NaN  tt2637294                en   \n",
       "\n",
       "           original_title                                           overview  \\\n",
       "0  Hot Tub Time Machine 2  When Lou, who has become the \"father of the In...   \n",
       "\n",
       "   popularity                       poster_path  ...  \\\n",
       "0    6.575393  /tQtWuwvMf0hCc2QR2tkolwl7c3c.jpg  ...   \n",
       "\n",
       "                                production_countries release_date runtime  \\\n",
       "0  [{'iso_3166_1': 'US', 'name': 'United States o...      2/20/15    93.0   \n",
       "\n",
       "                           spoken_languages    status  \\\n",
       "0  [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n",
       "\n",
       "                                             tagline                   title  \\\n",
       "0  The Laws of Space and Time are About to be Vio...  Hot Tub Time Machine 2   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [{'id': 4379, 'name': 'time travel'}, {'id': 9...   \n",
       "\n",
       "                                                cast  \\\n",
       "0  [{'cast_id': 4, 'character': 'Lou', 'credit_id...   \n",
       "\n",
       "                                                crew  \n",
       "0  [{'credit_id': '59ac067c92514107af02c8c8', 'de...  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_orignal.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这边总共21个特征，我们分别对每个特征进行分析，看其是否会与票房预测有帮助，剔除掉无帮助的。\n",
    "\n",
    "belongs_to_collection: 电影所属的集合，从其内容来看，应该是电影所属的题材，这边对其的处理，就是简单的是否有集合，有则为1，无则为0。\n",
    "\n",
    "budget: 预算，显然这对预测票房是有影响，预算的高低，会影响制作影片的质量。这边需要做的就是标准化处理，我们这里使用log1p标准化，确保不为0。\n",
    "\n",
    "genres: 电影的类型，比如科幻，动作，爱情等，这对于不同口味的人，也同样重要，也会影响票房。因为这个特征是对于不同类型电影是有id的，这个可以作为不同类别的区分，但有个问题，就是不同可能存在多个类别，我们需要将其拆开，分别处理。\n",
    "\n",
    "homepage: 是否有电影主页，这边也处理成有则为1，没有则为0。\n",
    "\n",
    "imdb_id: 这个是电影数据库对应的id，似乎对预测无影响，这里先舍弃掉！\n",
    "\n",
    "original_language: 电影的原始语言，有英语，韩语，中文等，这也多少会对结果预测有影响，这就对它做Onehotencoding操作。\n",
    "\n",
    "original_title: 这是电影原来的名字，这对于不同国家的电影，相对来说还是比较难断定是否与结果预测有影响的，本来有些人可能会有特殊喜好，一个好的电影名字来决定是否要去看一看这电影的。这里，我们试着去找有意义的特征，比如时间，年份，看还是算了，比较难找特征...\n",
    "\n",
    "overview: 电影的概括，这个特征也比较难表示，可能有些人也会读电影的简介，了解后，才去决定是否要去看，这个比较因人而异，比如定义是否有趣，这个怎么说呢，也比较主观，无法既定，那这边就简单以有无概述为特征，1为有，0为无。\n",
    "\n",
    "popularity: 电影的评分，这个比较客观，比之前的电影名和概述都直观，可度量性比较强，这里需要做的也是标准化处理，还是使用log1p标准化。\n",
    "\n",
    "poster_path: 这个电影的宣传海报，如果要找特征的话，就以什么类型的电影，应该有什么样的海报，是否达到预期，这感觉也比较主观，这里就以是否有海报来定吧，1为有，0为无。\n",
    "\n",
    "production_companies: 制片公司，这里包含的公司都有id，就以id来区分，也会有多个公司合作的情况，也需要分开处理。\n",
    "\n",
    "production_countries: 制片国家，与制片公司，同样处理，存在多国合拍的情况。\n",
    "\n",
    "release_date: 这是电影的发行日期，这可能对于有些人会认为过早的电影肯定也不咋的，就会选择不去看，那这边该如何定义这个特征呢？这边就以80年代之前，80到90年代，90到00，00到10，10到现在来进行区分。同时，我们还需要区分电影是在哪个季节上映的，这也会有一定的影响。\n",
    "\n",
    "runtime: 电影的时长，有些人可能没有耐心看长电影，这边以1个小时内，一个半小时，2个小时，2个小时到3个小时，3个小时以上来区分。\n",
    "\n",
    "spoken_languages: 影片中涉及到的语言，这也需要根据语言简写来区分，如kr, zh, en等。\n",
    "\n",
    "status: 是否发行，发行为1，没发行为0。\n",
    "\n",
    "tagline: 电影的宣传口号，这里就以是否有口号为特征，有则为1，无则为0。\n",
    "\n",
    "title: 而这才是正式使用的名字，这里我们可以以是否改名为特征。\n",
    "\n",
    "keywords: 有多个关键词的都可以分开区分。\n",
    "\n",
    "cast: 卡司，也就是演员阵容，这里主要关注有哪些演员参演，这对是否要去看是很重要的，故而会影响票房，所以这里每个cast中的演员id来区分，同时再加入出演次序，这可能会有影响！\n",
    "\n",
    "crew: 工作组人员信息，同样以cast中的工作人员id来区分，但感觉应该影响不大！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们再来看下需要附加数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:52.214344Z",
     "start_time": "2019-03-03T11:58:51.273803Z"
    }
   },
   "outputs": [],
   "source": [
    "tmdb_5000_movies = pd.read_csv('./数据集/tmdb_5000_movies.csv')\n",
    "tmdb_5000_credits = pd.read_csv('./数据集/tmdb_5000_credits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:52.223147Z",
     "start_time": "2019-03-03T11:58:52.217569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4803, 19)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmdb_5000_movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:52.232470Z",
     "start_time": "2019-03-03T11:58:52.225767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4803, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmdb_5000_credits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:52.248033Z",
     "start_time": "2019-03-03T11:58:52.235213Z"
    }
   },
   "outputs": [],
   "source": [
    "tmdb_5000 = pd.concat([tmdb_5000_movies, tmdb_5000_credits], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:52.258981Z",
     "start_time": "2019-03-03T11:58:52.250888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4803, 23)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmdb_5000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:52.269933Z",
     "start_time": "2019-03-03T11:58:52.261723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['budget', 'genres', 'homepage', 'id', 'keywords', 'original_language',\n",
       "       'original_title', 'overview', 'popularity', 'production_companies',\n",
       "       'production_countries', 'release_date', 'revenue', 'runtime',\n",
       "       'spoken_languages', 'status', 'tagline', 'vote_average', 'vote_count',\n",
       "       'movie_id', 'title', 'cast', 'crew'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmdb_5000.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:52.304270Z",
     "start_time": "2019-03-03T11:58:52.272738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>...</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>237000000</td>\n",
       "      <td>[{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...</td>\n",
       "      <td>http://www.avatarmovie.com/</td>\n",
       "      <td>19995</td>\n",
       "      <td>[{\"id\": 1463, \"name\": \"culture clash\"}, {\"id\":...</td>\n",
       "      <td>en</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>In the 22nd century, a paraplegic Marine is di...</td>\n",
       "      <td>150.437577</td>\n",
       "      <td>[{\"name\": \"Ingenious Film Partners\", \"id\": 289...</td>\n",
       "      <td>...</td>\n",
       "      <td>162.0</td>\n",
       "      <td>[{\"iso_639_1\": \"en\", \"name\": \"English\"}, {\"iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Enter the World of Pandora.</td>\n",
       "      <td>7.2</td>\n",
       "      <td>11800</td>\n",
       "      <td>19995</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>[{\"cast_id\": 242, \"character\": \"Jake Sully\", \"...</td>\n",
       "      <td>[{\"credit_id\": \"52fe48009251416c750aca23\", \"de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      budget                                             genres  \\\n",
       "0  237000000  [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...   \n",
       "\n",
       "                      homepage     id  \\\n",
       "0  http://www.avatarmovie.com/  19995   \n",
       "\n",
       "                                            keywords original_language  \\\n",
       "0  [{\"id\": 1463, \"name\": \"culture clash\"}, {\"id\":...                en   \n",
       "\n",
       "  original_title                                           overview  \\\n",
       "0         Avatar  In the 22nd century, a paraplegic Marine is di...   \n",
       "\n",
       "   popularity                               production_companies  ... runtime  \\\n",
       "0  150.437577  [{\"name\": \"Ingenious Film Partners\", \"id\": 289...  ...   162.0   \n",
       "\n",
       "                                    spoken_languages    status  \\\n",
       "0  [{\"iso_639_1\": \"en\", \"name\": \"English\"}, {\"iso...  Released   \n",
       "\n",
       "                       tagline vote_average vote_count movie_id   title  \\\n",
       "0  Enter the World of Pandora.          7.2      11800    19995  Avatar   \n",
       "\n",
       "                                                cast  \\\n",
       "0  [{\"cast_id\": 242, \"character\": \"Jake Sully\", \"...   \n",
       "\n",
       "                                                crew  \n",
       "0  [{\"credit_id\": \"52fe48009251416c750aca23\", \"de...  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmdb_5000.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:52.313563Z",
     "start_time": "2019-03-03T11:58:52.307688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmdb_5000.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:52.324062Z",
     "start_time": "2019-03-03T11:58:52.318865Z"
    }
   },
   "outputs": [],
   "source": [
    "matched_features = []\n",
    "without_matched_features = []\n",
    "for i in X_train_orignal.columns:\n",
    "    i = i.lower()\n",
    "    if i in tmdb_5000.columns:\n",
    "        matched_features.append(i)\n",
    "    else:\n",
    "        without_matched_features.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T11:58:52.338992Z",
     "start_time": "2019-03-03T11:58:52.332087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['budget',\n",
       " 'genres',\n",
       " 'homepage',\n",
       " 'original_language',\n",
       " 'original_title',\n",
       " 'overview',\n",
       " 'popularity',\n",
       " 'production_companies',\n",
       " 'production_countries',\n",
       " 'release_date',\n",
       " 'runtime',\n",
       " 'spoken_languages',\n",
       " 'status',\n",
       " 'tagline',\n",
       " 'title',\n",
       " 'keywords',\n",
       " 'cast',\n",
       " 'crew']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matched_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.613Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_5000_part1 = tmdb_5000[matched_features]\n",
    "X_train_5000_part2 = pd.DataFrame(columns=without_matched_features)\n",
    "X_train_5000 = pd.concat([X_train_5000_part1, X_train_5000_part2], axis=1)\n",
    "X_train_5000 = X_train_5000[X_train_orignal.columns]\n",
    "y_train_5000 = tmdb_5000['revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.618Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4803, 21)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_5000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.621Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['belongs_to_collection', 'budget', 'genres', 'homepage', 'imdb_id',\n",
       "       'original_language', 'original_title', 'overview', 'popularity',\n",
       "       'poster_path', 'production_companies', 'production_countries',\n",
       "       'release_date', 'runtime', 'spoken_languages', 'status', 'tagline',\n",
       "       'title', 'keywords', 'cast', 'crew'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_5000.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.625Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train_orignal, X_train_5000], axis=0, ignore_index=True)\n",
    "X = pd.concat([X_train, X_test], axis=0, ignore_index=True)\n",
    "y_train = pd.concat([y_train_orignal, y_train_5000], axis=0, ignore_index=True)\n",
    "y_train = np.log1p(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.628Z"
    }
   },
   "outputs": [],
   "source": [
    "X.loc[X['belongs_to_collection'].notnull(), 'belongs_to_collection'] = 1\n",
    "X['belongs_to_collection'].fillna(0, inplace=True)\n",
    "X['budget'] = np.log1p(X['budget'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.632Z"
    }
   },
   "outputs": [],
   "source": [
    "genres_find_id = X['genres'].str.findall('[0-9]{1,}')\n",
    "genres_find_id.fillna(0, inplace=True)\n",
    "genres_id_lst = []\n",
    "for i in range(len(genres_find_id)):\n",
    "    if genres_find_id.loc[i] != 0:\n",
    "        for j in genres_find_id.loc[i]: \n",
    "            j = int(j)\n",
    "            if j not in genres_id_lst:\n",
    "                genres_id_lst.append(j)\n",
    "genres_features = pd.DataFrame(np.zeros((len(genres_find_id), len(genres_id_lst)), dtype='int'), columns=['genres_' + str(i) for i in sorted(genres_id_lst)])\n",
    "for i in range(len(genres_find_id)):\n",
    "    if genres_find_id.loc[i] != 0:\n",
    "        for j in genres_find_id.loc[i]:\n",
    "            j = int(j)\n",
    "            genres_features.loc[i]['genres_' + str(j)] = 1\n",
    "X.drop('genres', axis=1, inplace=True)\n",
    "X = pd.concat([X, genres_features], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.637Z"
    }
   },
   "outputs": [],
   "source": [
    "X.loc[X['homepage'].notnull(), 'homepage'] = 1\n",
    "X['homepage'].fillna(0, inplace=True)\n",
    "X.loc[X['imdb_id'].notnull(), 'imdb_id'] = 1\n",
    "X['imdb_id'].fillna(0, inplace=True)\n",
    "\n",
    "original_language = pd.get_dummies(X['original_language'], prefix='original_language')\n",
    "X.drop('original_language', axis=1, inplace=True)\n",
    "X = pd.concat([X, original_language], axis=1)\n",
    "\n",
    "X.loc[X['overview'].notnull(), 'overview'] = 1\n",
    "X['overview'].fillna(0, inplace=True)\n",
    "X['popularity'] = np.log1p(X['popularity'])\n",
    "X.loc[X['poster_path'].notnull(), 'poster_path'] = 1\n",
    "X['poster_path'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.640Z"
    }
   },
   "outputs": [],
   "source": [
    "production_companies_find_id = X['production_companies'].str.findall('[0-9]{1,}')\n",
    "production_companies_find_id.fillna(0, inplace=True)\n",
    "production_companies_id_lst = []\n",
    "for i in range(len(production_companies_find_id)):\n",
    "    if production_companies_find_id.loc[i] != 0:\n",
    "        for j in production_companies_find_id.loc[i]: \n",
    "            j = int(j)\n",
    "            if j not in production_companies_id_lst:\n",
    "                production_companies_id_lst.append(j)\n",
    "production_companies_features = pd.DataFrame(np.zeros((len(production_companies_find_id), len(production_companies_id_lst)), dtype='int'), columns=['production_companies_' + str(i) for i in sorted(production_companies_id_lst)])\n",
    "for i in range(len(production_companies_find_id)):\n",
    "    if production_companies_find_id.loc[i] != 0:\n",
    "        for j in production_companies_find_id.loc[i]:\n",
    "            j = int(j)\n",
    "            production_companies_features.loc[i]['production_companies_' + str(j)] = 1\n",
    "X.drop('production_companies', axis=1, inplace=True)\n",
    "X = pd.concat([X, production_companies_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.643Z"
    }
   },
   "outputs": [],
   "source": [
    "production_countries_find_id = X['production_countries'].str.findall('[0-9]{1,}')\n",
    "production_countries_find_id.fillna(0, inplace=True)\n",
    "production_countries_id_lst = []\n",
    "for i in range(len(production_countries_find_id)):\n",
    "    if production_countries_find_id.loc[i] != 0:\n",
    "        for j in production_countries_find_id.loc[i]: \n",
    "            j = int(j)\n",
    "            if j not in production_countries_id_lst:\n",
    "                production_countries_id_lst.append(j)\n",
    "production_countries_features = pd.DataFrame(np.zeros((len(production_countries_find_id), len(production_countries_id_lst)), dtype='int'), columns=['production_countries_' + str(i) for i in sorted(production_countries_id_lst)])\n",
    "for i in range(len(production_countries_find_id)):\n",
    "    if production_countries_find_id.loc[i] != 0:\n",
    "        for j in production_countries_find_id.loc[i]:\n",
    "            j = int(j)\n",
    "            production_countries_features.loc[i]['production_countries_' + str(j)] = 1\n",
    "X.drop('production_countries', axis=1, inplace=True)\n",
    "X = pd.concat([X, production_countries_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.647Z"
    }
   },
   "outputs": [],
   "source": [
    "X['release_date'].fillna('2014-06-01', inplace=True)\n",
    "X['release_year'] = pd.to_datetime(X['release_date']).dt.year\n",
    "X['release_month'] = pd.to_datetime(X['release_date']).dt.month\n",
    "X.drop('release_date', axis=1, inplace=True)\n",
    "\n",
    "X.loc[X['release_year'] < 1980, 'release_time'] = 'before 80'\n",
    "X.loc[(X['release_year'] >= 1980) & (X['release_year'] < 1990), 'release_time'] = '80 to 90'\n",
    "X.loc[(X['release_year'] >= 1990) & (X['release_year'] < 2000), 'release_time'] = '90 to 00'\n",
    "X.loc[(X['release_year'] >= 2000) & (X['release_year'] < 2010), 'release_time'] = '00 to 10'\n",
    "X.loc[(X['release_year'] >= 2010) & (X['release_year'] <= int(time.strftime('%Y', time.localtime(time.time())))), 'release_time'] = '10 to now'\n",
    "X.drop('release_year', axis=1, inplace=True)\n",
    "\n",
    "X.loc[(X['release_month'] == 12) | (X['release_month'] == 1) | (X['release_month'] == 2), 'release_season'] = 'winter'\n",
    "X.loc[(X['release_month'] == 3) | (X['release_month'] == 4) | (X['release_month'] == 5), 'release_season'] = 'spring'\n",
    "X.loc[(X['release_month'] == 6) | (X['release_month'] == 7) | (X['release_month'] == 8), 'release_season'] = 'summer'\n",
    "X.loc[(X['release_month'] == 9) | (X['release_month'] == 10) | (X['release_month'] == 11), 'release_season'] = 'autumn'\n",
    "X.drop('release_month', axis=1, inplace=True)\n",
    "\n",
    "release_time = pd.get_dummies(X['release_time'], prefix='release_time')\n",
    "X.drop('release_time', axis=1, inplace=True)\n",
    "X = pd.concat([X, release_time], axis=1)\n",
    "\n",
    "release_season = pd.get_dummies(X['release_season'], prefix='release_season')\n",
    "X.drop('release_season', axis=1, inplace=True)\n",
    "X = pd.concat([X, release_season], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.651Z"
    }
   },
   "outputs": [],
   "source": [
    "X.loc[X['runtime'] <= 60, 'runtime_interval'] = 'within one hour'\n",
    "X.loc[(X['runtime'] > 60) & (X['runtime'] <= 90), 'runtime_interval'] = 'within one and a half hour'\n",
    "X.loc[(X['runtime'] > 90) & (X['runtime'] <= 120), 'runtime_interval'] = 'within two hours'\n",
    "X.loc[(X['runtime'] > 120) & (X['runtime'] <= 180), 'runtime_interval'] = 'within three hours'\n",
    "X.loc[X['runtime'] > 180, 'runtime_interval'] = 'more than three hours'\n",
    "\n",
    "X.drop('runtime', axis=1, inplace=True)\n",
    "runtime_interval = pd.get_dummies(X['runtime_interval'], prefix='runtime_interval')\n",
    "X.drop('runtime_interval', axis=1, inplace=True)\n",
    "X = pd.concat([X, runtime_interval], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.654Z"
    }
   },
   "outputs": [],
   "source": [
    "spoken_languages_find_lang = X['spoken_languages'].str.findall('\\'[a-z]{2}\\'')\n",
    "spoken_languages_find_lang.fillna(0, inplace=True)\n",
    "spoken_languages_lang_lst = []\n",
    "for i in range(len(spoken_languages_find_lang)):\n",
    "    if spoken_languages_find_lang.loc[i] != 0:\n",
    "        for j in spoken_languages_find_lang.loc[i]: \n",
    "            j = j[1:3]\n",
    "            if j not in spoken_languages_lang_lst:\n",
    "                spoken_languages_lang_lst.append(j)\n",
    "spoken_languages_features = pd.DataFrame(np.zeros((len(spoken_languages_find_lang), len(spoken_languages_lang_lst)), dtype='int'), columns=['spoken_languages_' + i for i in sorted(spoken_languages_lang_lst)])\n",
    "for i in range(len(spoken_languages_find_lang)):\n",
    "    if spoken_languages_find_lang.loc[i] != 0:\n",
    "        for j in spoken_languages_find_lang.loc[i]:\n",
    "            j = j[1:3]\n",
    "            spoken_languages_features.loc[i]['spoken_languages_' + j] = 1\n",
    "X.drop('spoken_languages', axis=1, inplace=True)\n",
    "X = pd.concat([X, spoken_languages_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.657Z"
    }
   },
   "outputs": [],
   "source": [
    "X.loc[X['status'].notnull(), 'status'] = 1\n",
    "X['status'].fillna(0, inplace=True)\n",
    "\n",
    "X.loc[X['tagline'].notnull(), 'tagline'] = 1\n",
    "X['tagline'].fillna(0, inplace=True)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if X.loc[i, 'original_title'] == X.loc[i, 'title']:\n",
    "        X.loc[i, 'title_changed'] = 1\n",
    "    else:\n",
    "        X.loc[i, 'title_changed'] = 0\n",
    "X['title_changed'] = X['title_changed'].astype('int')\n",
    "X.drop(['original_title', 'title'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.661Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_find_id = X['keywords'].str.findall('[0-9]{1,}')\n",
    "keywords_find_id.fillna(0, inplace=True)\n",
    "keywords_id_lst = []\n",
    "for i in range(len(keywords_find_id)):\n",
    "    if keywords_find_id.loc[i] != 0:\n",
    "        for j in keywords_find_id.loc[i]: \n",
    "            j = int(j)\n",
    "            if j not in keywords_id_lst:\n",
    "                keywords_id_lst.append(j)\n",
    "keywords_features = pd.DataFrame(np.zeros((len(keywords_find_id), len(keywords_id_lst)), dtype='int'), columns=['keywords_' + str(i) for i in sorted(keywords_id_lst)])\n",
    "for i in range(len(keywords_find_id)):\n",
    "    if keywords_find_id.loc[i] != 0:\n",
    "        for j in keywords_find_id.loc[i]:\n",
    "            j = int(j)\n",
    "            keywords_features.loc[i]['keywords_' + str(j)] = 1\n",
    "X.drop('keywords', axis=1, inplace=True)\n",
    "X = pd.concat([X, keywords_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.665Z"
    }
   },
   "outputs": [],
   "source": [
    "cast_find_actor_id = X['cast'].str.findall(\"\\'id\\'\\:\\s\\d{1,}\")\n",
    "cast_actor_id = []\n",
    "cast_find_actor_id.fillna(0, inplace=True)\n",
    "for i in range(len(cast_find_actor_id)):\n",
    "    if cast_find_actor_id.loc[i] != 0:\n",
    "        for j in cast_find_actor_id.loc[i]:\n",
    "            j = int(re.findall('[0-9]{1,}', j)[0])\n",
    "            if j not in cast_actor_id:\n",
    "                cast_actor_id.append(j)\n",
    "cast_actor_features = pd.DataFrame(np.zeros((len(cast_find_actor_id), len(cast_actor_id)), dtype='int'), columns=['cast_actor_' + str(i) for i in sorted(cast_actor_id)])\n",
    "for i in range(len(cast_find_actor_id)):\n",
    "    if cast_find_actor_id.loc[i] != 0:\n",
    "        for j in cast_find_actor_id.loc[i]:\n",
    "            j = int(re.findall('[0-9]{1,}', j)[0])\n",
    "            cast_actor_features.loc[i]['cast_actor_' + str(j)] = 1\n",
    "X = pd.concat([X, cast_actor_features], axis=1)\n",
    "\n",
    "cast_find_order_id = X['cast'].str.findall(\"\\'order\\'\\:\\s\\d{1,}\")\n",
    "cast_order_id = []\n",
    "cast_find_order_id.fillna(0, inplace=True)\n",
    "for i in range(len(cast_find_order_id)):\n",
    "    if cast_find_order_id.loc[i] != 0:\n",
    "        for j in cast_find_order_id.loc[i]:\n",
    "            j = int(re.findall('[0-9]{1,}', j)[0])\n",
    "            if j not in cast_order_id:\n",
    "                cast_order_id.append(j)\n",
    "cast_order_features = pd.DataFrame(np.zeros((len(cast_find_order_id), len(cast_order_id)), dtype='int'), columns=['cast_order_' + str(i) for i in sorted(cast_order_id)])\n",
    "for i in range(len(cast_find_order_id)):\n",
    "    if cast_find_order_id.loc[i] != 0:\n",
    "        for j in cast_find_order_id.loc[i]:\n",
    "            j = int(re.findall('[0-9]{1,}', j)[0])\n",
    "            cast_order_features.loc[i]['cast_order_' + str(j)] = 1\n",
    "X = pd.concat([X, cast_order_features], axis=1)\n",
    "X.drop('cast', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.669Z"
    }
   },
   "outputs": [],
   "source": [
    "crew_find_id = X['crew'].str.findall('[0-9]{1,}')\n",
    "crew_find_id.fillna(0, inplace=True)\n",
    "crew_id_lst = []\n",
    "for i in range(len(crew_find_id)):\n",
    "    if crew_find_id.loc[i] != 0:\n",
    "        for j in crew_find_id.loc[i]: \n",
    "            j = int(j)\n",
    "            if j not in crew_id_lst:\n",
    "                crew_id_lst.append(j)\n",
    "crew_features = pd.DataFrame(np.zeros((len(crew_find_id), len(crew_id_lst)), dtype='int'), columns=['crew_' + str(i) for i in sorted(crew_id_lst)])\n",
    "for i in range(len(crew_find_id)):\n",
    "    if crew_find_id.loc[i] != 0:\n",
    "        for j in crew_find_id.loc[i]:\n",
    "            j = int(j)\n",
    "            crew_features.loc[i]['crew_' + str(j)] = 1\n",
    "X.drop('crew', axis=1, inplace=True)\n",
    "X = pd.concat([X, crew_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.673Z"
    }
   },
   "outputs": [],
   "source": [
    "min_outlier_boundary = y_train.mean() - 3 * y_train.std()\n",
    "max_outlier_boundary = y_train.mean() + 3 * y_train.std()\n",
    "len(y_train[(y_train < min_outlier_boundary) | (y_train > max_outlier_boundary)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.677Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.681Z"
    }
   },
   "outputs": [],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:58:47.685Z"
    }
   },
   "outputs": [],
   "source": [
    "X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T11:56:20.916Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X[:len(X_train)]\n",
    "X_test = X[len(X_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T10:26:02.678Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain, Xval, ytrain, yval = train_test_split(X_train, y_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T05:55:26.376Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(n_jobs=-1)\n",
    "lr.fit(Xtrain, ytrain)\n",
    "ypred = lr.predict(Xval)\n",
    "np.sqrt(mean_squared_error(yval, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T05:55:26.395Z"
    }
   },
   "outputs": [],
   "source": [
    "enet = ElasticNet()\n",
    "gscv_enet = GridSearchCV(enet, {'alpha': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "                                'l1_ratio': [.1, .5, .7, .9, .95, .99, 1],\n",
    "                                'max_iter': [100, 200, 500, 1000, 2000, 5000],\n",
    "                                'tol': [1e-4, 1e-5, 1e-6],\n",
    "                                'random_state': range(501),\n",
    "                                'selection': ['random', 'cyclic']}, cv=10, n_jobs=-1, iid=False)\n",
    "gscv_enet.fit(Xtrain, ytrain)\n",
    "enet = ElasticNet(alpha=gscv_enet.best_params_['alpha'], l1_ratio=gscv_enet.best_params_['l1_ratio'],\n",
    "                  max_iter=gscv_enet.best_params_['max_iter'], tol=gscv_enet.best_params_['tol'],\n",
    "                  random_state=gscv_enet.best_params_['random_state'], selection=gscv_enet.best_params_['selection'])\n",
    "enet.fit(Xtrain, ytrain)\n",
    "ypred = enet.predict(Xval)\n",
    "np.sqrt(mean_squared_error(yval, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T05:55:26.402Z"
    }
   },
   "outputs": [],
   "source": [
    "svm_r = SVR()\n",
    "gscv_svr = GridSearchCV(svm_r, {'C': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "                                'kernel': ['linear', 'rbf'],\n",
    "                                'tol': [1e-3, 1e-4, 1e-5, 1e-6],\n",
    "                                'random_state': range(501)}, cv=10, n_jobs=-1, iid=False)\n",
    "gscv_svr.fit(Xtrain, ytrain)\n",
    "svm_r = SVR(C=gscv_svr.best_params_['C'], kernel=gscv_svr.best_params_['kernel'],\n",
    "            tol=gscv_svr.best_params_['tol'], random_state=gscv_svr.best_params_['random_state'])\n",
    "svm_r.fit(Xtrain, ytrain)\n",
    "ypred = svm_r.predict(Xval)\n",
    "np.sqrt(mean_squared_error(yval, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T05:55:26.419Z"
    }
   },
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor()\n",
    "gscv_rfr = GridSearchCV(rfr, {'n_estimators': [50, 80, 100, 200],\n",
    "                              'max_depth': [3, 10, 30, 50],\n",
    "                              'min_samples_split': [2, 5, 10],\n",
    "                              'min_samples_leaf': [1, 2, 5],\n",
    "                              'random_state': range(101)}, cv=10, n_jobs=-1, iid=False)\n",
    "gscv_rfr.fit(Xtrain, ytrain)\n",
    "rfr = RandomForestRegressor(n_estimators=gscv_rfr.best_params_['n_estimators'],\n",
    "                            max_depth=gscv_rfr.best_params_['max_depth'],\n",
    "                            min_samples_split=gscv_rfr.best_params_['min_samples_split'],\n",
    "                            min_samples_leaf=gscv_rfr.best_params_['min_samples_leaf'],\n",
    "                            random_state=gscv_rfr.best_params_['random_state'])\n",
    "rfr.fit(Xtrain, ytrain)\n",
    "ypred = rfr.predict(Xval)\n",
    "np.sqrt(mean_squared_error(yval, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T05:55:26.424Z"
    }
   },
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor()\n",
    "gscv_gbr = GridSearchCV(gbr, {'n_estimators': [50, 80, 100, 200],\n",
    "                              'max_depth': [3, 5, 10, 30, 50],\n",
    "                              'learning_rate': [0.05, 0.1, 0.2],\n",
    "                              'min_samples_split': [2, 5, 10],\n",
    "                              'min_samples_leaf': [1, 2, 5],\n",
    "                              'subsample': [0.6, 0.8, 1.0],\n",
    "                              'random_state': range(101)}, cv=10, n_jobs=-1, iid=False)\n",
    "gscv_gbr.fit(Xtrain, ytrain)\n",
    "gbr = GradientBoostingRegressor(loss='exponential',\n",
    "                                n_estimators=gscv_gbr.best_params_['n_estimators'],\n",
    "                                max_depth=gscv_gbr.best_params_['max_depth'],\n",
    "                                learning_rate=gscv_gbr.best_params_['learning_rate'],\n",
    "                                min_samples_split=gscv_gbr.best_params_['min_samples_split'],\n",
    "                                min_samples_leaf=gscv_gbr.best_params_['min_samples_leaf'],\n",
    "                                subsample=gscv_gbr.best_params_['subsample'],\n",
    "                                random_state=gscv_gbr.best_params_['random_state'])\n",
    "gbr.fit(Xtrain, ytrain)\n",
    "ypred = gbr.predict(Xval)\n",
    "np.sqrt(mean_squared_error(yval, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T05:55:26.432Z"
    }
   },
   "outputs": [],
   "source": [
    "xgbr = xgb.XGBRegressor()\n",
    "gscv_xgbr = GridSearchCV(xgbr, {'n_estimators': [50, 80, 100, 200],\n",
    "                                'max_depth': [3, 5, 10, 30, 50],\n",
    "                                'learning_rate': [0.05, 0.1, 0.2],\n",
    "                                'min_child_weight': [3, 5, 7, 9],\n",
    "                                'subsample': [0.6, 0.8, 1.0],\n",
    "                                'colsample_bytree': [0.6, 0.8, 0.1],\n",
    "                                'reg_lambda': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
    "                                'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "                                'random_state': range(101)}, cv=10, n_jobs=-1, iid=False)\n",
    "gscv_xgbr.fit(Xtrain, ytrain)\n",
    "xgbr = xgb.XGBRegressor(n_estimators=gscv_xgbr.best_params_['n_estimators'],\n",
    "                        max_depth=gscv_xgbr.best_params_['max_depth'],\n",
    "                        learning_rate=gscv_xgbr.best_params_['learning_rate'],\n",
    "                        min_child_weight=gscv_xgbr.best_params_['min_child_weight'],\n",
    "                        subsample=gscv_xgbr.best_params_['subsample'],\n",
    "                        colsample_bytree=gscv_xgbr.best_params_['colsample_bytree'],\n",
    "                        reg_lambda=gscv_xgbr.best_params_['reg_lambda'],\n",
    "                        reg_alpha=gscv_xgbr.best_params_['reg_alpha'],\n",
    "                        random_state=gscv_xgbr.best_params_['random_state'])\n",
    "xgbr.fit(Xtrain, ytrain)\n",
    "ypred = xgbr.predict(Xval)\n",
    "np.sqrt(mean_squared_error(yval, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T05:55:26.438Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import layers, models, callbacksbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T05:55:26.447Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain = np.array(Xtrain)\n",
    "Xval = np.array(Xval)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T05:55:26.485Z"
    }
   },
   "outputs": [],
   "source": [
    "min_val_mae = []\n",
    "params = []\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(patience=50) \n",
    "\n",
    "for hidden_size1 in [64, 128, 256, 512]:\n",
    "    for hidden_size2 in [64, 128, 256, 512]:\n",
    "        for activation in ['elu', 'selu', 'relu']:\n",
    "            for epochs in [10, 50, 100, 200]:\n",
    "                for batch_size in [100, 200, 500, 1000]:\n",
    "                    model = models.Sequential()\n",
    "                    model.add(layers.Dense(hidden_size1, activation=activation))\n",
    "                    model.add(layers.Dense(hidden_size2, activation=activation))\n",
    "                    model.add(layers.Dense(1))\n",
    "\n",
    "                    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "                    history = model.fit(Xtrain, ytrain, epochs=epochs, batch_size=batch_size, validation_data=(Xval, yval), verbose=0, callbacks=[early_stopping])\n",
    "                    \n",
    "                    min_val_mae.append(min(history.history['val_mae']))\n",
    "                    params.append([hidden_size1, hidden_size2, activation, epochs, batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-03T05:55:26.492Z"
    }
   },
   "outputs": [],
   "source": [
    "best_params = params[np.argmin(min_val_mae)]\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(best_params[0], activation=best_params[2]))\n",
    "model.add(layers.Dense(best_params[1], activation=best_params[2]))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.fit(Xtrain, ytrain, epochs=best_params[3], batch_size=best_params[4], validation_data=(Xval, yval), callbacks=[early_stopping])\n",
    "\n",
    "ypred = model.predict(Xval)\n",
    "np.sqrt(mean_squared_error(yval, ypred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
