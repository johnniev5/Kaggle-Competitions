{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "说明：这里使用IMDB电影数据集，该数据集包含50000条训练和测试数据，共3个字段，id，sentiment，review，sentiment字段属于label标签，review是文本评价内容，这个评价内容最多不超过30条评价信息，sentimen的评价标准，就是打分平均低于5分，则为0，大于7分的，则为1。这里的关键就是找到一种文本内容表示的方法，常用的有词袋模型，而对于情感分析，存在前后的语义联系，单纯地使用词袋模型，是不够的，故而采用词向量表示法，同时结合双向RNN来实现。\n",
    "\n",
    "* id - Unique ID of each review\n",
    "* sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "* review - Text of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:17:08.848687Z",
     "start_time": "2019-02-28T08:17:07.338056Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:17:08.860134Z",
     "start_time": "2019-02-28T08:17:08.852681Z"
    }
   },
   "outputs": [],
   "source": [
    "def loadata(file, train=True):\n",
    "    if train:\n",
    "        data_train = pd.read_csv(file, delimiter='\\t')\n",
    "        return data_train\n",
    "    else:\n",
    "        data_test = pd.read_csv(file, delimiter='\\t')\n",
    "        return data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:17:10.685154Z",
     "start_time": "2019-02-28T08:17:08.867800Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train = loadata('./数据集/labeledTrainData.tsv')\n",
    "data_test = loadata('./数据集/testData.tsv', train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T09:41:26.088795Z",
     "start_time": "2019-02-18T09:41:26.080795Z"
    }
   },
   "source": [
    "对比词向量的表示，这边打算尝试两种方法：\n",
    "\n",
    "1. 从头开始训练，更加贴近任务本身；\n",
    "2. 使用训练好的wordvec来微调词向量；\n",
    "\n",
    "两者做一个比较！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:17:10.729008Z",
     "start_time": "2019-02-28T08:17:10.698194Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = data_train['review']\n",
    "y_train = data_train['sentiment']\n",
    "\n",
    "x_test = data_test['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:17:10.746159Z",
     "start_time": "2019-02-28T08:17:10.735302Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:17:10.768629Z",
     "start_time": "2019-02-28T08:17:10.749896Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocab(x_train, x_test, max_features):\n",
    "    x = pd.concat([x_train, x_test], ignore_index=True)\n",
    "    counter = Counter()\n",
    "    for i in range(len(x)):\n",
    "        text = x.loc[i].replace('<br /><br />', ' ').replace(\"¨\", \"\").lower()\n",
    "        text = re.sub(\"\\d+\", '9', text)\n",
    "        text = re.sub(\"((?=[\\\\x21-\\\\x7e]+)[^A-Za-z0-9])\", \"\", text)\n",
    "        text = text.strip().split(' ')\n",
    "        for word in text:\n",
    "            counter[word] += 1\n",
    "    word2idx = {word[0]:i+2 for i, word in enumerate(counter.most_common(max_features))}\n",
    "    word2idx['PAD'] = 0\n",
    "    word2idx['UNK'] = 1\n",
    "    idx2word = {v:k for k, v in word2idx.items()}\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:17:10.783986Z",
     "start_time": "2019-02-28T08:17:10.771625Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_maxlen(x_train, x_test):\n",
    "    x = pd.concat([x_train, x_test], ignore_index=True)\n",
    "    sent_maxlen = 0\n",
    "    for i in range(len(x)):\n",
    "        text = x.loc[i].replace('<br /><br />', ' ').replace(\"¨\", \"\").lower()\n",
    "        text = re.sub(\"\\d+\", '9', text)\n",
    "        text = re.sub(\"((?=[\\\\x21-\\\\x7e]+)[^A-Za-z0-9])\", \"\", text).strip()\n",
    "        sent_len = len(text.split(' '))\n",
    "        if sent_len > sent_maxlen:\n",
    "            sent_maxlen = sent_len\n",
    "    return sent_maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:17:10.805603Z",
     "start_time": "2019-02-28T08:17:10.793113Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize(x_train, x_test, word2idx):\n",
    "    x = pd.concat([x_train, x_test], ignore_index=True)\n",
    "    input_seqs = []\n",
    "    for i in range(len(x)):\n",
    "        input_seq = []\n",
    "        text = x.loc[i].replace('<br /><br />', ' ').replace(\"¨\", \"\").lower()\n",
    "        text = re.sub(\"\\d+\", '9', text)\n",
    "        text = re.sub(\"((?=[\\\\x21-\\\\x7e]+)[^A-Za-z0-9])\", \"\", text)\n",
    "        text = text.strip().split(' ')\n",
    "        for word in text:\n",
    "            if word in word2idx.keys():\n",
    "                input_seq.append(word2idx[word])\n",
    "            else:\n",
    "                input_seq.append(word2idx['UNK'])\n",
    "        input_seqs.append(input_seq)\n",
    "    return input_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T08:17:16.038709Z",
     "start_time": "2019-02-28T08:17:10.811902Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnnie/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers, models, callbacks\n",
    "import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-28T08:17:07.364Z"
    }
   },
   "outputs": [],
   "source": [
    "max_val_acc = []\n",
    "params = []\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(patience=50) \n",
    "\n",
    "for max_features in [10000, 20000, 30000, 40000, 50000]:\n",
    "    word2idx, idx2word = build_vocab(x_train, x_test, max_features)\n",
    "    vocab_size = min(max_features, len(word2idx)) + 2\n",
    "    input_seqs = vectorize(x_train, x_test, word2idx)\n",
    "    for maxlen in [100, 200, 500, 1000]:\n",
    "        seqlen = min(get_maxlen(x_train, x_test), maxlen)\n",
    "        X = pad_sequences(input_seqs, maxlen=seqlen)\n",
    "        X_train, X_test = X[:len(x_train)], X[len(x_train):]\n",
    "        Xtrain, Xval, ytrain, yval = train_test_split(X_train, y_train)\n",
    "        for embeding_size in [128, 300]:\n",
    "            for hidden_size1 in [64, 128, 256, 512]:\n",
    "                for hidden_size2 in [64, 128, 256, 512]:\n",
    "                    for dropout1 in [0.2, 0.3, 0.5, 0.8]:\n",
    "                        for dropout2 in [0.2, 0.3, 0.5, 0.8]:\n",
    "                            for l1 in [0.001, 0.005, 0.01, 0.02, 0.05, 0.1]:\n",
    "                                for l2 in [0.001, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2]:\n",
    "                                    for batch_size in [100, 300, 500, 1000]:\n",
    "                                        for epochs in [10, 30, 50, 100, 200]:\n",
    "                                            model = models.Sequential()\n",
    "                                            model.add(layers.Embedding(vocab_size, embeding_size, input_length=seqlen))\n",
    "                                            model.add(layers.LSTM(hidden_size1, dropout=dropout1, recurrent_dropout=dropout2, return_sequences=True, kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "                                            model.add(layers.LSTM(hidden_size2, dropout=dropout1, recurrent_dropout=dropout2, kernel_regularizer=keras.regularizers.l1_l2(l1=l1, l2=l2)))\n",
    "                                            model.add(layers.Dense(1, activation='sigmoid'))\n",
    "                                            model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "          \n",
    "                                            history = model.fit(Xtrain, ytrain, validation_data=(Xval, yval), batch_size=batch_size, epochs=epochs, verbose=0, callbacks=[early_stopping])\n",
    "                                            max_val_acc.append(max(history.history['val_acc']))\n",
    "                                            params.append([max_features, maxlen, embeding_size, hidden_size1, hidden_size2, dropout1, dropout2, l1, l2, batch_size, epochs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-28T08:17:07.367Z"
    }
   },
   "outputs": [],
   "source": [
    "best_params = params[np.argmax(max_val_acc)]\n",
    "\n",
    "word2idx, idx2word = build_vocab(x_train, x_test, best_params[0])\n",
    "vocab_size = min(best_params[0], len(word2idx)) + 2\n",
    "input_seqs = vectorize(x_train, x_test, word2idx)\n",
    "\n",
    "seqlen = min(get_maxlen(x_train, x_test), best_params[1])\n",
    "X = pad_sequences(input_seqs, maxlen=seqlen)\n",
    "X_train, X_test = X[:len(x_train)], X[len(x_train):]\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(X_train, y_train)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(vocab_size, best_params[2], input_length=seqlen))\n",
    "model.add(layers.LSTM(best_params[3], dropout=best_params[5], recurrent_dropout=best_params[6], return_sequences=True, kernel_regularizer=keras.regularizers.l1_l2(l1=best_params[7], l2=best_params[8])))\n",
    "model.add(layers.LSTM(best_params[4], dropout=best_params[5], recurrent_dropout=best_params[6], kernel_regularizer=keras.regularizers.l1_l2(l1=best_params[7], l2=best_params[8])))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(Xtrain, ytrain, validation_data=(Xval, yval), batch_size=best_params[9], epochs=best_params[10], verbose=0, callbacks=[early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
